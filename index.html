<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Multiple Imputation Through XGBoost • mixgb</title>
<script src="deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet">
<script src="deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><link href="deps/font-awesome-6.5.2/css/all.min.css" rel="stylesheet">
<link href="deps/font-awesome-6.5.2/css/v4-shims.min.css" rel="stylesheet">
<script src="deps/headroom-0.11.0/headroom.min.js"></script><script src="deps/headroom-0.11.0/jQuery.headroom.min.js"></script><script src="deps/bootstrap-toc-1.0.1/bootstrap-toc.min.js"></script><script src="deps/clipboard.js-2.0.11/clipboard.min.js"></script><script src="deps/search-1.0.0/autocomplete.jquery.min.js"></script><script src="deps/search-1.0.0/fuse.min.js"></script><script src="deps/search-1.0.0/mark.min.js"></script><!-- pkgdown --><script src="pkgdown.js"></script><meta property="og:title" content="Multiple Imputation Through XGBoost">
<meta name="description" content="Multiple imputation using XGBoost, subsampling, and predictive mean matching as described in Deng and Lumley (2023) &lt;doi:10.1080/10618600.2023.2252501&gt;. The package supports various types of variables, offers flexible settings, and enables saving an imputation model to impute new data. Data processing and memory usage have been optimised to speed up the imputation process.">
<meta property="og:description" content="Multiple imputation using XGBoost, subsampling, and predictive mean matching as described in Deng and Lumley (2023) &lt;doi:10.1080/10618600.2023.2252501&gt;. The package supports various types of variables, offers flexible settings, and enables saving an imputation model to impute new data. Data processing and memory usage have been optimised to speed up the imputation process.">
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>


    <nav class="navbar navbar-expand-lg fixed-top bg-light" data-bs-theme="light" aria-label="Site navigation"><div class="container">

    <a class="navbar-brand me-2" href="index.html">mixgb</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">1.5.3</small>


    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="nav-item"><a class="nav-link" href="reference/index.html">Reference</a></li>
<li class="nav-item dropdown">
  <button class="nav-link dropdown-toggle" type="button" id="dropdown-articles" data-bs-toggle="dropdown" aria-expanded="false" aria-haspopup="true">Articles</button>
  <ul class="dropdown-menu" aria-labelledby="dropdown-articles">
<li><a class="dropdown-item" href="articles/Imputing-newdata.html">Imputing newdata with a saved mixgb imputer</a></li>
    <li><a class="dropdown-item" href="articles/Using-mixgb.html">mixgb: Multiple Imputation Through XGBoost</a></li>
  </ul>
</li>
<li class="nav-item"><a class="nav-link" href="news/index.html">Changelog</a></li>
      </ul>
<ul class="navbar-nav">
<li class="nav-item"><form class="form-inline" role="search">
 <input class="form-control" type="search" name="search-input" id="search-input" autocomplete="off" aria-label="Search site" placeholder="Search for" data-search-index="search.json">
</form></li>
<li class="nav-item"><a class="external-link nav-link" href="https://github.com/agnesdeng/mixgb/" aria-label="GitHub"><span class="fa fab fa-github fa-lg"></span></a></li>
      </ul>
</div>


  </div>
</nav><div class="container template-home">
<div class="row">
  <main id="main" class="col-md-9"><div class="section level1">
<div class="page-header"><h1 id="mixgb">mixgb<a class="anchor" aria-label="anchor" href="#mixgb"></a>
</h1></div>
<!-- badges: start -->

<p>The R package <strong>mixgb</strong> provides a scalable approach for multiple imputation by leveraging XGBoost, subsampling and predictive mean matching. We have shown that our method can yield less biased estimates and reflect appropriate imputation variability, while achieving high computational efficiency. For further information, please refer to our paper <a href="https://yongshideng.com/journal-article/mixgb/" class="external-link">Multiple Imputation Through XGBoost</a>.</p>
<div class="section level2">
<h2 id="references">References<a class="anchor" aria-label="anchor" href="#references"></a>
</h2>
<ul>
<li><p>Yongshi Deng &amp; Thomas Lumley. (2023), <a href="https://yongshideng.com/journal-article/mixgb/" class="external-link">Multiple Imputation Through XGBoost</a>, Journal of Computational and Graphical Statistics, 33(2), 352-363. DOI: 10.1080/10618600.2023.2252501.</p></li>
<li><p>Tianqi Chen &amp; Carlos Guestrin. (2016), <a href="http://arxiv.org/abs/1603.02754" class="external-link">XGBoost: A Scalable Tree Boosting System</a>, In 22nd SIGKDD Conference on Knowledge Discovery and Data Mining.</p></li>
</ul>
</div>
<div class="section level2">
<h2 id="new-updates">New updates<a class="anchor" aria-label="anchor" href="#new-updates"></a>
</h2>
<p><strong>New Release: Nov 2024</strong></p>
<ul>
<li><p>New CRAN version 1.5.2. A lot faster than 1.0.2!</p></li>
<li><p>Some visual diagnostic functions have been moved to the <code>vismi</code> package, which provides a wide range of visualisation tools for multiple imputation. For more details, please check the <code>vismi</code> package on GitHub <a href="https://github.com/agnesdeng/vismi" class="external-link">Visualisation Tools for Multiple Imputation</a>.</p></li>
</ul>
<p><strong>Nov 2023</strong></p>
<ul>
<li><p>New version of the <code><a href="reference/mixgb.html">mixgb()</a></code> function has been optimized to greatly reduce imputation time for large datasets.</p></li>
<li><p><strong>mixgb</strong>(&gt;=1.4.2) is compatible with both XGBoost(&gt;=2.0.0) and XGBoost(CRAN version).</p></li>
</ul>
<p><strong>Oct 2023</strong></p>
<ul>
<li>Dependency Change: Starting from mixgb version 1.3.1, the package requires XGBoost version 2.0.0 or higher. As of now, this version is not available on CRAN. To get the required version with GPU support, please download it from <a href="https://github.com/dmlc/xgboost/releases" class="external-link">XGBoost GitHub Releases</a>. Then in R, one can install XGBoost 2.0.0 and the newest version of <strong>mixgb</strong> as follows:</li>
</ul>
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Change the file path where you saved the downloaded XGBoost package</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/install.packages.html" class="external-link">install.packages</a></span><span class="op">(</span><span class="st">"path_to_downloaded_file/xgboost_r_gpu_win64_2.0.0.tar.gz"</span>, repos <span class="op">=</span> <span class="cn">NULL</span><span class="op">)</span></span></code></pre></div>
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu">devtools</span><span class="fu">::</span><span class="fu">install_github</span><span class="op">(</span><span class="st">"agnesdeng/mixgb"</span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/agnesdeng/mixgb" class="external-link">mixgb</a></span><span class="op">)</span></span></code></pre></div>
<ul>
<li><p>If you prefer to use the CRAN version of XGBoost, consider using an earlier version of <strong>mixgb</strong> (versions &lt;1.3.1).</p></li>
<li>
<p>Now compatible with XGBoost 2.0.0! To align with XGBoost 2.0.0, <strong>mixgb</strong> introduces new parameter <code>device</code> and removed parameters<code>gpu_id</code> and <code>predictor</code>. Also, <code>tree_method="hist"</code> by default.</p>
<ul>
<li><p><code>mixgb(device="cpu", tree_method="hist",.....)</code></p></li>
<li><p><code>mixgb(device="cuda", tree_method="hist",.....)</code></p></li>
</ul>
</li>
<li><p>Now support saving imputation models in a local directory in JSON format.</p></li>
</ul>
<p><strong>May 2023</strong></p>
<ul>
<li>Support logical data automatically without the need to convert it to factor type.</li>
</ul>
<p>Now <code>mixgb(data,...)</code> support a dataset with the following data types:</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode R"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" tabindex="-1"></a><span class="sc">-</span> numeric</span>
<span id="cb3-2"><a href="#cb3-2" tabindex="-1"></a></span>
<span id="cb3-3"><a href="#cb3-3" tabindex="-1"></a><span class="sc">-</span> integer</span>
<span id="cb3-4"><a href="#cb3-4" tabindex="-1"></a></span>
<span id="cb3-5"><a href="#cb3-5" tabindex="-1"></a><span class="sc">-</span> factor</span>
<span id="cb3-6"><a href="#cb3-6" tabindex="-1"></a></span>
<span id="cb3-7"><a href="#cb3-7" tabindex="-1"></a><span class="sc">-</span> logical</span></code></pre></div>
<p>Please note that variables of <code>character</code> type need to be manually converted to factor by users before imputation.</p>
<p><strong>January 2023</strong></p>
<ul>
<li>Major change of default settings for mixgb().</li>
</ul>
<p>Our package has changed from using bootstrapping to subsampling with a default setting of <code>subsample = 0.7</code>. This decision is based on the discovery that although bootstrapping is generally effectively, it can introduce bias in certain scenarios. As a result, subsampling has been adopted as the default approach.</p>
<p><strong>May 2022</strong></p>
<ul>
<li>Introduce visual diagnostic functions for multiply imputed data.</li>
<li>Use S3 instead of R6.</li>
<li>Plot functions can now show masked missing data (if provided).</li>
</ul>
<p><strong>April 2022</strong></p>
<ul>
<li>User can adjust the number of iterations with the <code>maxit</code> parameter.</li>
<li>Both single and multiple imputation with XGBoost can use predictive mean matching.</li>
<li>Bootstrapping data to obtain <code>m</code> imputations is optional. Users can set <code>bootstrap = FALSE</code> to disable bootstrap. Users can also set sampling-related hyperparameters of XGBoost (<code>subsample</code>, <code>colsample_bytree</code>, <code>colsample_bylevel</code>, <code>colsample_bynode</code>) to be less than 1 to achieve a similar effect.</li>
<li>Add predicted mean matching type 0. Now the options for <code>pmm.type</code> are <code>NULL</code>,<code>0</code>,<code>1</code>,<code>2</code> or <code>"auto"</code> (type 2 for numeric/integer variables, no PMM for categorical variables).</li>
<li>Add more validation checks.</li>
<li>Compatible with <code>data.table</code>.</li>
<li>Add function <code><a href="reference/mixgb_cv.html">mixgb_cv()</a></code> to pre-tune <code>nrounds</code> by cross-validation.</li>
</ul>
</div>
<div class="section level2">
<h2 id="under-development">Under development<a class="anchor" aria-label="anchor" href="#under-development"></a>
</h2>
<ul>
<li>In progress: To Be Announced</li>
<li>Planned: To Be Announced</li>
<li>Under consideration: To implement PMM type 3</li>
</ul>
</div>
<div class="section level2">
<h2 id="notice">Notice<a class="anchor" aria-label="anchor" href="#notice"></a>
</h2>
<ul>
<li>For multithreading, users can set the XGBoost nthread parameter with OpenMP support. Be advised, OpenMP support is currently disabled on MacOS.</li>
</ul>
</div>
<div class="section level2">
<h2 id="id_1-installation">1. Installation<a class="anchor" aria-label="anchor" href="#id_1-installation"></a>
</h2>
<p>You can install the development version of mixgb from <a href="https://github.com/" class="external-link">GitHub</a> with:</p>
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># install.packages("devtools")</span></span>
<span><span class="fu">devtools</span><span class="fu">::</span><span class="fu">install_github</span><span class="op">(</span><span class="st">"agnesdeng/mixgb"</span><span class="op">)</span></span></code></pre></div>
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># load mixgb</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/agnesdeng/mixgb" class="external-link">mixgb</a></span><span class="op">)</span></span></code></pre></div>
<div class="section level3">
<h3 id="id_11-data-cleaning-before-imputation">1.1 Data cleaning before imputation<a class="anchor" aria-label="anchor" href="#id_11-data-cleaning-before-imputation"></a>
</h3>
<p>It is highly recommended to clean and check your data before imputation. Here are some common issues:</p>
<ul>
<li>Data should be a data frame.</li>
<li>ID should be removed</li>
<li>Missing values should be coded as <code>NA</code> not <code>NaN</code>
</li>
<li>
<code>Inf</code> or <code>-Inf</code> are not allowed</li>
<li>Empty cells should be coded as <code>NA</code> or sensible values</li>
<li>Variables of “character” type should be converted to “factor” instead</li>
<li>Variables of “factor” type should have at least two levels</li>
</ul>
<p>The function <code><a href="reference/data_clean.html">data_clean()</a></code> serves the purpose of performing a preliminary check and fix some evident issues. However, the function cannot resolve all data quality-related problems.</p>
<div class="sourceCode" id="cb6"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">cleanWithNA.df</span> <span class="op">&lt;-</span> <span class="fu"><a href="reference/data_clean.html">data_clean</a></span><span class="op">(</span><span class="va">rawdata</span><span class="op">)</span></span></code></pre></div>
</div>
</div>
<div class="section level2">
<h2 id="id_2-impute-missing-values-with-mixgb">2. Impute missing values with <code>mixgb</code>
<a class="anchor" aria-label="anchor" href="#id_2-impute-missing-values-with-mixgb"></a>
</h2>
<p>We first load the <code>mixgb</code> package and the <code>nhanes3_newborn</code> dataset, which contains 16 variables of various types (integer/numeric/factor/ordinal factor). There are 9 variables with missing values.</p>
<div class="sourceCode" id="cb7"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/utils/str.html" class="external-link">str</a></span><span class="op">(</span><span class="va">nhanes3_newborn</span><span class="op">)</span></span>
<span><span class="co">#&gt; tibble [2,107 × 16] (S3: tbl_df/tbl/data.frame)</span></span>
<span><span class="co">#&gt;  $ HSHSIZER: int [1:2107] 4 3 5 4 4 3 5 3 3 3 ...</span></span>
<span><span class="co">#&gt;  $ HSAGEIR : int [1:2107] 2 5 10 10 8 3 10 7 2 7 ...</span></span>
<span><span class="co">#&gt;  $ HSSEX   : Factor w/ 2 levels "1","2": 2 1 2 2 1 1 2 2 2 1 ...</span></span>
<span><span class="co">#&gt;  $ DMARACER: Factor w/ 3 levels "1","2","3": 1 1 2 1 1 1 2 1 2 2 ...</span></span>
<span><span class="co">#&gt;  $ DMAETHNR: Factor w/ 3 levels "1","2","3": 3 1 3 3 3 3 3 3 3 3 ...</span></span>
<span><span class="co">#&gt;  $ DMARETHN: Factor w/ 4 levels "1","2","3","4": 1 3 2 1 1 1 2 1 2 2 ...</span></span>
<span><span class="co">#&gt;  $ BMPHEAD : num [1:2107] 39.3 45.4 43.9 45.8 44.9 42.2 45.8 NA 40.2 44.5 ...</span></span>
<span><span class="co">#&gt;   ..- attr(*, "label")= chr "Head circumference (cm)"</span></span>
<span><span class="co">#&gt;  $ BMPRECUM: num [1:2107] 59.5 69.2 69.8 73.8 69 61.7 74.8 NA 64.5 70.2 ...</span></span>
<span><span class="co">#&gt;   ..- attr(*, "label")= chr "Recumbent length (cm)"</span></span>
<span><span class="co">#&gt;  $ BMPSB1  : num [1:2107] 8.2 13 6 8 8.2 9.4 5.2 NA 7 5.9 ...</span></span>
<span><span class="co">#&gt;   ..- attr(*, "label")= chr "First subscapular skinfold (mm)"</span></span>
<span><span class="co">#&gt;  $ BMPSB2  : num [1:2107] 8 13 5.6 10 7.8 8.4 5.2 NA 7 5.4 ...</span></span>
<span><span class="co">#&gt;   ..- attr(*, "label")= chr "Second subscapular skinfold (mm)"</span></span>
<span><span class="co">#&gt;  $ BMPTR1  : num [1:2107] 9 15.6 7 16.4 9.8 9.6 5.8 NA 11 6.8 ...</span></span>
<span><span class="co">#&gt;   ..- attr(*, "label")= chr "First triceps skinfold (mm)"</span></span>
<span><span class="co">#&gt;  $ BMPTR2  : num [1:2107] 9.4 14 8.2 12 8.8 8.2 6.6 NA 10.9 7.6 ...</span></span>
<span><span class="co">#&gt;   ..- attr(*, "label")= chr "Second triceps skinfold (mm)"</span></span>
<span><span class="co">#&gt;  $ BMPWT   : num [1:2107] 6.35 9.45 7.15 10.7 9.35 7.15 8.35 NA 7.35 8.65 ...</span></span>
<span><span class="co">#&gt;   ..- attr(*, "label")= chr "Weight (kg)"</span></span>
<span><span class="co">#&gt;  $ DMPPIR  : num [1:2107] 3.186 1.269 0.416 2.063 1.464 ...</span></span>
<span><span class="co">#&gt;   ..- attr(*, "label")= chr "Poverty income ratio"</span></span>
<span><span class="co">#&gt;  $ HFF1    : Factor w/ 2 levels "1","2": 2 2 1 1 1 2 2 1 2 1 ...</span></span>
<span><span class="co">#&gt;  $ HYD1    : Ord.factor w/ 5 levels "1"&lt;"2"&lt;"3"&lt;"4"&lt;..: 1 3 1 1 1 1 1 1 2 1 ...</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/colSums.html" class="external-link">colSums</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/NA.html" class="external-link">is.na</a></span><span class="op">(</span><span class="va">nhanes3_newborn</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; HSHSIZER  HSAGEIR    HSSEX DMARACER DMAETHNR DMARETHN  BMPHEAD BMPRECUM </span></span>
<span><span class="co">#&gt;        0        0        0        0        0        0      124      114 </span></span>
<span><span class="co">#&gt;   BMPSB1   BMPSB2   BMPTR1   BMPTR2    BMPWT   DMPPIR     HFF1     HYD1 </span></span>
<span><span class="co">#&gt;      161      169      124      167      117      192        7        0</span></span></code></pre></div>
<p>To impute this dataset, we can use the default settings. The default number of imputed datasets is <code>m = 5</code>. Note that we do not need to convert our data into dgCMatrix or one-hot coding format. Our package will automatically convert it for you. Variables should be of the following types: numeric, integer, factor or ordinal factor.</p>
<div class="sourceCode" id="cb8"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># use mixgb with default settings</span></span>
<span><span class="va">imputed.data</span> <span class="op">&lt;-</span> <span class="fu"><a href="reference/mixgb.html">mixgb</a></span><span class="op">(</span>data <span class="op">=</span> <span class="va">nhanes3_newborn</span>, m <span class="op">=</span> <span class="fl">5</span><span class="op">)</span></span></code></pre></div>
<div class="section level3">
<h3 id="id_21-customize-imputation-settings">2.1 Customize imputation settings<a class="anchor" aria-label="anchor" href="#id_21-customize-imputation-settings"></a>
</h3>
<p>We can also customize imputation settings:</p>
<ul>
<li><p>The number of imputed datasets <code>m</code></p></li>
<li><p>The number of imputation iterations <code>maxit</code></p></li>
<li><p>XGBoost hyperparameters and verbose settings. <code>xgb.params</code>, <code>nrounds</code>, <code>early_stopping_rounds</code>, <code>print_every_n</code> and <code>verbose</code>.</p></li>
<li><p>Subsampling ratio. By default, <code>subsample = 0.7</code>. Users can change this value under the <code>xgb.params</code> argument.</p></li>
<li><p>Predictive mean matching settings <code>pmm.type</code>, <code>pmm.k</code> and <code>pmm.link</code>.</p></li>
<li><p>Whether ordinal factors should be converted to integer (imputation process may be faster) <code>ordinalAsInteger</code></p></li>
<li><p>Whether or not to use bootstrapping <code>bootstrap</code></p></li>
<li><p>Initial imputation methods for different types of variables <code>initial.num</code>, <code>initial.int</code> and <code>initial.fac</code>.</p></li>
<li><p>Whether to save models for imputing newdata <code>save.models</code> and <code>save.vars</code>.</p></li>
</ul>
<div class="sourceCode" id="cb9"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Use mixgb with chosen settings</span></span>
<span><span class="va">params</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span></span>
<span>  max_depth <span class="op">=</span> <span class="fl">5</span>,</span>
<span>  subsample <span class="op">=</span> <span class="fl">0.9</span>,</span>
<span>  nthread <span class="op">=</span> <span class="fl">2</span>,</span>
<span>  tree_method <span class="op">=</span> <span class="st">"hist"</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="va">imputed.data</span> <span class="op">&lt;-</span> <span class="fu"><a href="reference/mixgb.html">mixgb</a></span><span class="op">(</span></span>
<span>  data <span class="op">=</span> <span class="va">nhanes3_newborn</span>, m <span class="op">=</span> <span class="fl">10</span>, maxit <span class="op">=</span> <span class="fl">2</span>,</span>
<span>  ordinalAsInteger <span class="op">=</span> <span class="cn">FALSE</span>, bootstrap <span class="op">=</span> <span class="cn">FALSE</span>,</span>
<span>  pmm.type <span class="op">=</span> <span class="st">"auto"</span>, pmm.k <span class="op">=</span> <span class="fl">5</span>, pmm.link <span class="op">=</span> <span class="st">"prob"</span>,</span>
<span>  initial.num <span class="op">=</span> <span class="st">"normal"</span>, initial.int <span class="op">=</span> <span class="st">"mode"</span>, initial.fac <span class="op">=</span> <span class="st">"mode"</span>,</span>
<span>  save.models <span class="op">=</span> <span class="cn">FALSE</span>, save.vars <span class="op">=</span> <span class="cn">NULL</span>,</span>
<span>  xgb.params <span class="op">=</span> <span class="va">params</span>, nrounds <span class="op">=</span> <span class="fl">200</span>, early_stopping_rounds <span class="op">=</span> <span class="fl">10</span>, print_every_n <span class="op">=</span> <span class="fl">10L</span>, verbose <span class="op">=</span> <span class="fl">0</span></span>
<span><span class="op">)</span></span></code></pre></div>
</div>
<div class="section level3">
<h3 id="id_22-tune-hyperparameters">2.2 Tune hyperparameters<a class="anchor" aria-label="anchor" href="#id_22-tune-hyperparameters"></a>
</h3>
<p>Imputation performance can be affected by the hyperparameter settings. Although tuning a large set of hyperparameters may appear intimidating, it is often possible to narrowing down the search space because many hyperparameters are correlated. In our package, the function <code><a href="reference/mixgb_cv.html">mixgb_cv()</a></code> can be used to tune the number of boosting rounds - <code>nrounds</code>. There is no default <code>nrounds</code> value in <code>XGBoost,</code> so users are required to specify this value themselves. The default <code>nrounds</code> in <code><a href="reference/mixgb.html">mixgb()</a></code> is 100. However, we recommend using <code><a href="reference/mixgb_cv.html">mixgb_cv()</a></code> to find the optimal <code>nrounds</code> first.</p>
<div class="sourceCode" id="cb10"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">params</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span>max_depth <span class="op">=</span> <span class="fl">3</span>, subsample <span class="op">=</span> <span class="fl">0.7</span>, nthread <span class="op">=</span> <span class="fl">2</span><span class="op">)</span></span>
<span><span class="va">cv.results</span> <span class="op">&lt;-</span> <span class="fu"><a href="reference/mixgb_cv.html">mixgb_cv</a></span><span class="op">(</span>data <span class="op">=</span> <span class="va">nhanes3_newborn</span>, nrounds <span class="op">=</span> <span class="fl">100</span>, xgb.params <span class="op">=</span> <span class="va">params</span>, verbose <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span></span>
<span><span class="va">cv.results</span><span class="op">$</span><span class="va">response</span></span>
<span><span class="co">#&gt; [1] "BMPSB2"</span></span>
<span><span class="va">cv.results</span><span class="op">$</span><span class="va">best.nrounds</span></span>
<span><span class="co">#&gt; [1] 16</span></span></code></pre></div>
<p>By default, <code><a href="reference/mixgb_cv.html">mixgb_cv()</a></code> will randomly choose an incomplete variable as the response and build an XGBoost model with other variables as explanatory variables using the complete cases of the dataset. Therefore, each run of <code><a href="reference/mixgb_cv.html">mixgb_cv()</a></code> will likely return different results. Users can also specify the response and covariates in the argument <code>response</code> and <code>select_features</code> respectively.</p>
<div class="sourceCode" id="cb11"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">cv.results</span> <span class="op">&lt;-</span> <span class="fu"><a href="reference/mixgb_cv.html">mixgb_cv</a></span><span class="op">(</span></span>
<span>  data <span class="op">=</span> <span class="va">nhanes3_newborn</span>, nfold <span class="op">=</span> <span class="fl">10</span>, nrounds <span class="op">=</span> <span class="fl">100</span>, early_stopping_rounds <span class="op">=</span> <span class="fl">1</span>,</span>
<span>  response <span class="op">=</span> <span class="st">"BMPHEAD"</span>, select_features <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"HSAGEIR"</span>, <span class="st">"HSSEX"</span>, <span class="st">"DMARETHN"</span>, <span class="st">"BMPRECUM"</span>, <span class="st">"BMPSB1"</span>, <span class="st">"BMPSB2"</span>, <span class="st">"BMPTR1"</span>, <span class="st">"BMPTR2"</span>, <span class="st">"BMPWT"</span><span class="op">)</span>, xgb.params <span class="op">=</span> <span class="va">params</span>, verbose <span class="op">=</span> <span class="cn">FALSE</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="va">cv.results</span><span class="op">$</span><span class="va">best.nrounds</span></span>
<span><span class="co">#&gt; [1] 18</span></span></code></pre></div>
<p>Let us just try setting <code>nrounds = cv.results$best.nrounds</code> in <code><a href="reference/mixgb.html">mixgb()</a></code> to obtain 5 imputed datasets.</p>
<div class="sourceCode" id="cb12"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">imputed.data</span> <span class="op">&lt;-</span> <span class="fu"><a href="reference/mixgb.html">mixgb</a></span><span class="op">(</span>data <span class="op">=</span> <span class="va">nhanes3_newborn</span>, m <span class="op">=</span> <span class="fl">5</span>, nrounds <span class="op">=</span> <span class="va">cv.results</span><span class="op">$</span><span class="va">best.nrounds</span><span class="op">)</span></span></code></pre></div>
</div>
</div>
<div class="section level2">
<h2 id="id_3-visualize-multiply-imputed-values">3. Visualize multiply imputed values<a class="anchor" aria-label="anchor" href="#id_3-visualize-multiply-imputed-values"></a>
</h2>
<p>It is crucial to assess the plausibility of imputations before doing an analysis.</p>
<p>The <code>mixgb</code> package used to provide a few visual diagnostics functions. However, we have moved these functions to the <code>vismi</code> package, which provides a wide range of visualisation tools for multiple imputation.</p>
<p>For more details, please check the <code>vismi</code> package on GitHub <a href="https://github.com/agnesdeng/vismi" class="external-link">Visualisation Tools for Multiple Imputation</a>.</p>
</div>
<div class="section level2">
<h2 id="id_4-impute-new-unseen-data-using-a-saved-imputer-object">4. Impute new unseen data using a saved imputer object<a class="anchor" aria-label="anchor" href="#id_4-impute-new-unseen-data-using-a-saved-imputer-object"></a>
</h2>
<p>To demonstrate how to impute new data using a saved imputer, we first split the <code>nhanes3_newborn</code> dataset into training data and test data.</p>
<div class="sourceCode" id="cb13"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/Random.html" class="external-link">set.seed</a></span><span class="op">(</span><span class="fl">2022</span><span class="op">)</span></span>
<span><span class="va">n</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/nrow.html" class="external-link">nrow</a></span><span class="op">(</span><span class="va">nhanes3</span><span class="op">)</span></span>
<span><span class="va">idx</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/sample.html" class="external-link">sample</a></span><span class="op">(</span><span class="fl">1</span><span class="op">:</span><span class="va">n</span>, size <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/Round.html" class="external-link">round</a></span><span class="op">(</span><span class="fl">0.7</span> <span class="op">*</span> <span class="va">n</span><span class="op">)</span>, replace <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span></span>
<span><span class="va">train.data</span> <span class="op">&lt;-</span> <span class="va">nhanes3</span><span class="op">[</span><span class="va">idx</span>, <span class="op">]</span></span>
<span><span class="va">test.data</span> <span class="op">&lt;-</span> <span class="va">nhanes3</span><span class="op">[</span><span class="op">-</span><span class="va">idx</span>, <span class="op">]</span></span></code></pre></div>
<p>Next we impute the training data using <code><a href="reference/mixgb.html">mixgb()</a></code>. We can use the training data to generate <code>m</code> imputed datasets and save their imputation models. To achieve this, users need to set <code>save.models = TRUE</code>. By default, imputation models for all variables with missing values in the training data will be saved (<code>save.vars = NULL</code>). However, it is possible that unseen data may have missing values in other variables. To be thorough, users can save models for all variables by setting <code>save.vars = colnames(train.data)</code>. Note that this may take significantly longer as it requires training and saving a model for each variable. In cases where users are confident that only certain variables will have missing values in the new data, it is advisable to specify the names or indices of these variables in <code>save.vars</code> rather than saving models for all variables.</p>
<p>To save the imputer object, users need to specify a local directory in the parameter <code>save.models.folder</code> in the main function <code><a href="reference/mixgb.html">mixgb()</a></code>. Models will be save as JSON format by calling <code>xgb.save()</code> internally. Saving XGBoost models in this way instead of using <code>saveRDS</code> in R is recommended by XGBoost. This can ensure that the imputation models can still be used in later release of XGBoost.</p>
<div class="sourceCode" id="cb14"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># obtain m imputed datasets for train.data and save imputation models</span></span>
<span><span class="va">mixgb.obj</span> <span class="op">&lt;-</span> <span class="fu"><a href="reference/mixgb.html">mixgb</a></span><span class="op">(</span>data <span class="op">=</span> <span class="va">train.data</span>, m <span class="op">=</span> <span class="fl">5</span>, save.models <span class="op">=</span> <span class="cn">TRUE</span>, save.models.folder <span class="op">=</span> <span class="st">"C:/Users/....."</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/readRDS.html" class="external-link">saveRDS</a></span><span class="op">(</span>object <span class="op">=</span> <span class="va">mixgb.obj</span>, file <span class="op">=</span> <span class="st">"C:/Users/.../mixgbimputer.rds"</span><span class="op">)</span></span></code></pre></div>
<p>If users specify the <code>save.models.folder</code>, the return object will include the following:</p>
<ul>
<li><p><code>imputed.data</code>: a list of <code>m</code> imputed datasets for training data</p></li>
<li><p><code>XGB.models</code>: a list of directories of <code>m</code> sets of XGBoost models for variables specified in <code>save.vars</code>.</p></li>
<li><p><code>params</code>: a list of parameters that are required for imputing new data using <code><a href="reference/impute_new.html">impute_new()</a></code> later on.</p></li>
<li><p><code>XGB.save</code> : a parameter indicates whether <code>XGB.models</code> are the saved models or the directories for the saved models.</p></li>
</ul>
<p>As the <code>mixgb.obj</code> does not contain the models themselves, users need not worry about saving this object via <code><a href="https://rdrr.io/r/base/readRDS.html" class="external-link">saveRDS()</a></code>. For later use, one can load the object into R and impute new data.</p>
<p>To impute new data with this saved imputer object, we can use the <code><a href="reference/impute_new.html">impute_new()</a></code> function.</p>
<div class="sourceCode" id="cb15"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">mixgb.obj</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/readRDS.html" class="external-link">readRDS</a></span><span class="op">(</span>file <span class="op">=</span> <span class="st">"C:/Users/.../mixgbimputer.rds"</span><span class="op">)</span></span>
<span><span class="va">test.imputed</span> <span class="op">&lt;-</span> <span class="fu"><a href="reference/impute_new.html">impute_new</a></span><span class="op">(</span>object <span class="op">=</span> <span class="va">mixgb.obj</span>, newdata <span class="op">=</span> <span class="va">test.data</span><span class="op">)</span></span></code></pre></div>
<p>Users can choose whether to use new data for initial imputation. By default, the information of training data is used to initially impute the missing data in the new dataset (<code>initial.newdata = FALSE</code>). After this, the missing values in the new dataset will be imputed using the saved models from the imputer object. This process will be considerably faster because it does not involve rebuilding the imputation models.</p>
<div class="sourceCode" id="cb16"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">test.imputed</span> <span class="op">&lt;-</span> <span class="fu"><a href="reference/impute_new.html">impute_new</a></span><span class="op">(</span>object <span class="op">=</span> <span class="va">mixgb.obj</span>, newdata <span class="op">=</span> <span class="va">test.data</span><span class="op">)</span></span></code></pre></div>
<p>If PMM is used in <code><a href="reference/mixgb.html">mixgb()</a></code>, predicted values of missing entries in the new dataset will be matched with donors from the training data. Additionally, users can set the number of donors to be used in PMM when imputing new data. The default setting <code>pmm.k = NULL</code> indicates that the same setting as the training object will be used.</p>
<p>Similarly, users can set the number of imputed datasets <code>m</code> in <code><a href="reference/impute_new.html">impute_new()</a></code>. Note that this value has to be less than or equal to the <code>m</code> value specified in <code><a href="reference/mixgb.html">mixgb()</a></code>. If this value is not specified, the function will use the same <code>m</code> value as the saved object.</p>
<div class="sourceCode" id="cb17"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">test.imputed</span> <span class="op">&lt;-</span> <span class="fu"><a href="reference/impute_new.html">impute_new</a></span><span class="op">(</span>object <span class="op">=</span> <span class="va">mixgb.obj</span>, newdata <span class="op">=</span> <span class="va">test.data</span>, initial.newdata <span class="op">=</span> <span class="cn">FALSE</span>, pmm.k <span class="op">=</span> <span class="fl">3</span>, m <span class="op">=</span> <span class="fl">4</span><span class="op">)</span></span></code></pre></div>
</div>
<div class="section level2">
<h2 id="id_5-install-mixgb-with-gpu-support">5. Install <code>mixgb</code> with GPU support<a class="anchor" aria-label="anchor" href="#id_5-install-mixgb-with-gpu-support"></a>
</h2>
<p>Multiple imputation can be run with GPU support for machines with NVIDIA GPUs. Users must first install the R package <code>xgboost</code> with GPU support.</p>
<div class="section level3">
<h3 id="newest-version-xgboost--200-mixgb--131">Newest Version (XGBoost &gt;= 2.0.0, mixgb &gt;= 1.3.1)<a class="anchor" aria-label="anchor" href="#newest-version-xgboost--200-mixgb--131"></a>
</h3>
<p>Please download the Newest version of XGBoost with GPU support via <a href="https://github.com/dmlc/xgboost/releases" class="external-link">XGBoost GitHub Releases</a>.</p>
<div class="sourceCode" id="cb18"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Change the file path where you saved the downloaded XGBoost package</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/install.packages.html" class="external-link">install.packages</a></span><span class="op">(</span><span class="st">"path_to_downloaded_file/xgboost_r_gpu_win64_2.0.0.tar.gz"</span>, repos <span class="op">=</span> <span class="cn">NULL</span><span class="op">)</span></span></code></pre></div>
<p>Then users can install the newest version of our package <code>mixgb</code> in R.</p>
<div class="sourceCode" id="cb19"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu">devtools</span><span class="fu">::</span><span class="fu">install_github</span><span class="op">(</span><span class="st">"agnesdeng/mixgb"</span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/agnesdeng/mixgb" class="external-link">mixgb</a></span><span class="op">)</span></span></code></pre></div>
<p>To utilize the GPU version of mixgb(), users can simply specify <code>device = "cuda"</code> in the params list which will then be passed to the <code>xgb.params</code> argument in the function <code><a href="reference/mixgb.html">mixgb()</a></code>. Note that by default, <code>tree_method = "hist"</code> from XGBoost 2.0.0.</p>
<div class="sourceCode" id="cb20"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">params</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span></span>
<span>  device <span class="op">=</span> <span class="st">"cuda"</span>,</span>
<span>  subsample <span class="op">=</span> <span class="fl">0.7</span>,</span>
<span>  nthread <span class="op">=</span> <span class="fl">1</span>,</span>
<span>  tree_method <span class="op">=</span> <span class="st">"hist"</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="va">mixgb.data</span> <span class="op">&lt;-</span> <span class="fu"><a href="reference/mixgb.html">mixgb</a></span><span class="op">(</span>data <span class="op">=</span> <span class="va">withNA.df</span>, m <span class="op">=</span> <span class="fl">5</span>, xgb.params <span class="op">=</span> <span class="va">params</span><span class="op">)</span></span></code></pre></div>
</div>
<div class="section level3">
<h3 id="old-version-xgboost--200-mixgb--131">Old Version (XGBoost &lt; 2.0.0, mixgb &lt; 1.3.1)<a class="anchor" aria-label="anchor" href="#old-version-xgboost--200-mixgb--131"></a>
</h3>
<p>The <code>xgboost</code> R package pre-built binary on Linux x86_64 with GPU support can be downloaded from the release page <a href="https://github.com/dmlc/xgboost/releases/tag/v1.4.0" class="external-link uri">https://github.com/dmlc/xgboost/releases/tag/v1.4.0</a></p>
<p>The package can then be installed by running the following commands:</p>
<div class="sourceCode" id="cb21"><pre class="sourceCode R"><code class="sourceCode r"><span id="cb21-1"><a href="#cb21-1" tabindex="-1"></a><span class="co"># Install dependencies</span></span>
<span id="cb21-2"><a href="#cb21-2" tabindex="-1"></a><span class="sc">$</span> R <span class="sc">-</span>q <span class="sc">-</span>e <span class="st">"install.packages(c('data.table', 'jsonlite'))"</span></span>
<span id="cb21-3"><a href="#cb21-3" tabindex="-1"></a></span>
<span id="cb21-4"><a href="#cb21-4" tabindex="-1"></a><span class="co"># Install XGBoost</span></span>
<span id="cb21-5"><a href="#cb21-5" tabindex="-1"></a><span class="sc">$</span> R CMD INSTALL .<span class="sc">/</span>xgboost_r_gpu_linux.tar.gz</span></code></pre></div>
<p>Then users can install package <code>mixgb</code> in R.</p>
<div class="sourceCode" id="cb22"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu">devtools</span><span class="fu">::</span><span class="fu">install_github</span><span class="op">(</span><span class="st">"agnesdeng/mixgb"</span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/agnesdeng/mixgb" class="external-link">mixgb</a></span><span class="op">)</span></span></code></pre></div>
<p>To utilize the GPU version of mixgb(), users can simply specify <code>tree_method = "gpu_hist"</code> in the params list which will then be passed to the <code>xgb.params</code> argument in the function <code><a href="reference/mixgb.html">mixgb()</a></code>. Other adjustable GPU-related arguments include <code>gpu_id</code> and <code>predictor</code>. By default, <code>gpu_id = 0</code> and <code>predictor = "auto"</code>.</p>
<div class="sourceCode" id="cb23"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">params</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span></span>
<span>  max_depth <span class="op">=</span> <span class="fl">3</span>,</span>
<span>  subsample <span class="op">=</span> <span class="fl">0.7</span>,</span>
<span>  nthread <span class="op">=</span> <span class="fl">1</span>,</span>
<span>  tree_method <span class="op">=</span> <span class="st">"gpu_hist"</span>,</span>
<span>  gpu_id <span class="op">=</span> <span class="fl">0</span>,</span>
<span>  predictor <span class="op">=</span> <span class="st">"auto"</span></span>
<span><span class="op">)</span></span>
<span></span>
<span></span>
<span><span class="va">mixgb.data</span> <span class="op">&lt;-</span> <span class="fu"><a href="reference/mixgb.html">mixgb</a></span><span class="op">(</span>data <span class="op">=</span> <span class="va">withNA.df</span>, m <span class="op">=</span> <span class="fl">5</span>, xgb.params <span class="op">=</span> <span class="va">params</span><span class="op">)</span></span></code></pre></div>
</div>
</div>
</div>
  </main><aside class="col-md-3"><div class="links">
<h2 data-toc-skip>Links</h2>
<ul class="list-unstyled">
<li><a href="https://cloud.r-project.org/package=mixgb" class="external-link">View on CRAN</a></li>
<li><a href="https://github.com/agnesdeng/mixgb/" class="external-link">Browse source code</a></li>
<li><a href="https://github.com/agnesdeng/mixgb/issues" class="external-link">Report a bug</a></li>
</ul>
</div>

<div class="license">
<h2 data-toc-skip>License</h2>
<ul class="list-unstyled">
<li><a href="LICENSE.html">Full license</a></li>
<li><small>GPL (&gt;= 3)</small></li>
</ul>
</div>


<div class="citation">
<h2 data-toc-skip>Citation</h2>
<ul class="list-unstyled">
<li><a href="authors.html#citation">Citing mixgb</a></li>
</ul>
</div>

<div class="developers">
<h2 data-toc-skip>Developers</h2>
<ul class="list-unstyled">
<li>Yongshi Deng <br><small class="roles"> Author, maintainer </small> <a href="https://orcid.org/0000-0001-5845-859X" target="orcid.widget" aria-label="ORCID" class="external-link"><span class="fab fa-orcid orcid" aria-hidden="true"></span></a> </li>
<li><a href="authors.html">More about authors...</a></li>
</ul>
</div>

<div class="dev-status">
<h2 data-toc-skip>Dev status</h2>
<ul class="list-unstyled">
<li><a href="https://github.com/agnesdeng/mixgb" class="external-link"><img src="https://img.shields.io/badge/Made%20With-R-9cf" alt="Made with R"></a></li>
<li><a href="https://cran.r-project.org/package=mixgb" class="external-link"><img src="https://img.shields.io/cran/v/mixgb?color=9cf" alt="CRAN version"></a></li>
<li><a href="https://cran.r-project.org/package=mixgb" class="external-link"><img src="https://cranlogs.r-pkg.org/badges/mixgb" alt="CRAN downloads"></a></li>
<li><a href="https://github.com/agnesdeng/mixgb/releases" class="external-link"><img src="https://img.shields.io/github/v/release/agnesdeng/mixgb?color=green" alt="GitHub release (latest by date)"></a></li>
<li><a href="https://github.com/agnesdeng/mixgb/actions/workflows/R-CMD-check.yaml" class="external-link"><img src="https://github.com/agnesdeng/mixgb/actions/workflows/R-CMD-check.yaml/badge.svg" alt="R-CMD-check"></a></li>
</ul>
</div>

  </aside>
</div>


    <footer><div class="pkgdown-footer-left">
  <p>Developed by Yongshi Deng.</p>
</div>

<div class="pkgdown-footer-right">
  <p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.1.1.</p>
</div>

    </footer>
</div>





  </body>
</html>
